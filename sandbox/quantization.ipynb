{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight quantization using IBM aihwkit - PyTorch\n",
    "---\n",
    "\n",
    "In the following notebook, we will demonstrate how to quantize the weights of a given neural network using the IBM aihwkit library. We will use a model inspired by the ResNet9 architecture and the CIFAR-10 dataset. \n",
    "For simplicity, to implement the analog version of the model to be used by the simulator, we will use the auxiliary functions defined in the examples [18_cifar10_on_resnet.ipynb](../examples/18_cifar10_on_resnet.py), [30_external_hardware_aware_model.ipynb](../examples/30_external_hardware_aware_model.py) and the tutorial [hw_aware_training.ipynb](../notebooks/tutorial/hw_aware_training.ipynb)  form which this notebook takes ample inspiration. We will also show how to quantize the weights of a model converted from digital to analog using the [built-in functions](../src/aihwkit/nn/conversion.py) provided by the aihwkit library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a simple analog device composed of ***single device crossbars tiles*** and a ***weight quantization at int3***. For simplicity, consider the simulation to be carried out on the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://aihwkit-gpu-demo.s3.us-east.cloud-object-storage.appdomain.cloud/aihwkit-0.9.0+cuda117-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "# !pip install aihwkit-0.9.0+cuda117-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "# !pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'WeightQuantizerParameter' from 'aihwkit.simulator.parameters' (/Users/edoardocabiati/anaconda3/envs/aihwkit/lib/python3.12/site-packages/aihwkit/simulator/parameters/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maihwkit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimulator\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfigs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InferenceRPUConfig, TorchInferenceRPUConfig\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maihwkit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimulator\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpresets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PresetIOParameters\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maihwkit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimulator\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     25\u001b[0m     MappingParameter,\n\u001b[1;32m     26\u001b[0m     IOParameters,\n\u001b[1;32m     27\u001b[0m     PrePostProcessingParameter,\n\u001b[1;32m     28\u001b[0m     InputRangeParameter,\n\u001b[1;32m     29\u001b[0m     WeightClipParameter,\n\u001b[1;32m     30\u001b[0m     WeightRemapParameter,\n\u001b[1;32m     31\u001b[0m     WeightModifierParameter,\n\u001b[1;32m     32\u001b[0m     WeightQuantizerParameter,\n\u001b[1;32m     33\u001b[0m )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maihwkit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimulator\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameters\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menums\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     35\u001b[0m     WeightClipType,\n\u001b[1;32m     36\u001b[0m     BoundManagementType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     WeightModifierType,\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maihwkit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnalogLinearMapped, AnalogConv2dMapped, AnalogSequential, AnalogLinear\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'WeightQuantizerParameter' from 'aihwkit.simulator.parameters' (/Users/edoardocabiati/anaconda3/envs/aihwkit/lib/python3.12/site-packages/aihwkit/simulator/parameters/__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn, Tensor, device, no_grad, manual_seed\n",
    "from torch import nn\n",
    "from torchvision.datasets.utils import download_url\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.functional import accuracy\n",
    "import torchvision\n",
    "from torch.nn.functional import mse_loss\n",
    "\n",
    "# Import functions defined in a specific path\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "\n",
    "from aihwkit.simulator.configs import ConstantStepDevice, SingleRPUConfig\n",
    "from aihwkit.optim import AnalogSGD\n",
    "from aihwkit.inference.noise.base import BaseNoiseModel\n",
    "from aihwkit.inference.noise.pcm import PCMLikeNoiseModel\n",
    "from aihwkit.inference.compensation.drift import GlobalDriftCompensation\n",
    "from aihwkit.inference.compensation.base import BaseDriftCompensation\n",
    "from aihwkit.simulator.configs import InferenceRPUConfig, TorchInferenceRPUConfig\n",
    "from aihwkit.simulator.presets.utils import PresetIOParameters\n",
    "from aihwkit.simulator.parameters import (\n",
    "    MappingParameter,\n",
    "    IOParameters,\n",
    "    PrePostProcessingParameter,\n",
    "    InputRangeParameter,\n",
    "    WeightClipParameter,\n",
    "    WeightRemapParameter,\n",
    "    WeightModifierParameter,\n",
    "    WeightQuantizerParameter,\n",
    ")\n",
    "from aihwkit.simulator.parameters.enums import (\n",
    "    WeightClipType,\n",
    "    BoundManagementType,\n",
    "    NoiseManagementType,\n",
    "    WeightNoiseType,\n",
    "    WeightRemapType,\n",
    "    WeightModifierType,\n",
    ")\n",
    "from aihwkit.nn import AnalogLinearMapped, AnalogConv2dMapped, AnalogSequential, AnalogLinear\n",
    "# from aihwkit.utils.visualization import plot_device_compact\n",
    "# from aihwkit.utils.analog_info import analog_summary\n",
    "# from aihwkit.utils.fitting import fit_measurements\n",
    "from aihwkit.nn.conversion import convert_to_analog\n",
    "from aihwkit.utils.analog_info import analog_summary\n",
    "from aihwkit.inference.calibration import (\n",
    "    calibrate_input_ranges,\n",
    "    InputRangeCalibrationType,\n",
    ")\n",
    "#from aihwkit.simulator.rpu_base import cuda\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the quantization levels\n",
    "quantization_levels = 4\n",
    "assert quantization_levels & (quantization_levels - 1) == 0, \"quantization_levels must be a power of 2\"\n",
    "\n",
    "class LambdaLayer(torch.nn.Module):\n",
    "    def __init__(self, lambd):\n",
    "        super(LambdaLayer, self).__init__()\n",
    "        self.lambd = lambd\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lambd(x)\n",
    "\n",
    "# Definitions of some building blocks for the ResNet, taken form example '18_cifar10_on_resnet.ipynb'\n",
    "class BasicBlock(torch.nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, option=\"A\"):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False\n",
    "        )\n",
    "        self.bn1 = torch.nn.BatchNorm2d(planes)\n",
    "        self.conv2 = torch.nn.Conv2d(\n",
    "            planes, planes, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = torch.nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = torch.nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            if option == \"A\":\n",
    "                \"\"\"\n",
    "                For CIFAR10 ResNet paper uses option A.\n",
    "                \"\"\"\n",
    "                self.shortcut = LambdaLayer(\n",
    "                    lambda x: F.pad(\n",
    "                        x[:, :, ::2, ::2],\n",
    "                        (0, 0, 0, 0, planes // 4, planes // 4),\n",
    "                        \"constant\",\n",
    "                        0,\n",
    "                    )\n",
    "                )\n",
    "            elif option == \"B\":\n",
    "                self.shortcut = torch.nn.Sequential(\n",
    "                    torch.nn.Conv2d(\n",
    "                        in_planes,\n",
    "                        self.expansion * planes,\n",
    "                        kernel_size=1,\n",
    "                        stride=stride,\n",
    "                        bias=False,\n",
    "                    ),\n",
    "                    torch.nn.BatchNorm2d(self.expansion * planes),\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Resnet9(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    From https://github.com/matthias-wright/cifar10-resnet/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels):\n",
    "        super(Resnet9, self).__init__()\n",
    "\n",
    "        self.channels = channels\n",
    "\n",
    "        # resnet9 [56,112,224,224]\n",
    "        # resnet9s [28,28,28,56]\n",
    "\n",
    "        self.bn1 = torch.nn.BatchNorm2d(num_features=channels[0], momentum=0.9)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(num_features=channels[1], momentum=0.9)\n",
    "        self.bn3 = torch.nn.BatchNorm2d(num_features=channels[2], momentum=0.9)\n",
    "        self.bn4 = torch.nn.BatchNorm2d(num_features=channels[3], momentum=0.9)\n",
    "\n",
    "        self.conv = torch.nn.Sequential(\n",
    "            # prep\n",
    "            torch.nn.Conv2d(\n",
    "                in_channels=3,\n",
    "                out_channels=channels[0],\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            self.bn1,\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            # Layer 1\n",
    "            torch.nn.Conv2d(\n",
    "                in_channels=channels[0],\n",
    "                out_channels=channels[1],\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            self.bn2,\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            BasicBlock(in_planes=channels[1], planes=channels[1], stride=1),\n",
    "            # Layer 2\n",
    "            torch.nn.Conv2d(\n",
    "                in_channels=channels[1],\n",
    "                out_channels=channels[2],\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            self.bn3,\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Layer 3\n",
    "            torch.nn.Conv2d(\n",
    "                in_channels=channels[2],\n",
    "                out_channels=channels[3],\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            self.bn4,\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            BasicBlock(in_planes=channels[3], planes=channels[3], stride=1),\n",
    "            torch.nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "        )\n",
    "\n",
    "        self.fc = torch.nn.Linear(in_features=channels[3], out_features=10, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = out.view(-1, self.channels[3])\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "def resnet9s():\n",
    "    return Resnet9(channels=[28, 28, 28, 56])\n",
    "\n",
    "def createOuterBlock(InnerBlock, in_ch, hidden_ch ,n_internal_blocks, initial_stride):\n",
    "    strides = [initial_stride] + [1] * (n_internal_blocks - 1)\n",
    "    layers = []\n",
    "    for stride in strides:\n",
    "        layers.append(InnerBlock(in_ch, hidden_ch, stride=stride))\n",
    "        in_ch = hidden_ch\n",
    "    return AnalogSequential(*layers)\n",
    "\n",
    "def createResNet34(basic_block, list_num_blocks, out_classes=10):\n",
    "    in_ch = 16\n",
    "    conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    bn1 = nn.BatchNorm2d(16)\n",
    "    layer1 = createOuterBlock(basic_block, in_ch, 16, list_num_blocks[0], initial_stride=1)\n",
    "    layer2 = createOuterBlock(basic_block, in_ch, 32, list_num_blocks[1], initial_stride=2)\n",
    "    layer3 = createOuterBlock(basic_block, in_ch, 64, list_num_blocks[2], initial_stride=2)\n",
    "    fc = AnalogLinearMapped(64, out_classes)\n",
    "\n",
    "    return AnalogSequential(conv1, bn1, layer1, layer2, layer3, nn.AdaptiveAvgPool2d(1), nn.Flatten(), fc)\n",
    "\n",
    "\n",
    "def get_test_loader(batch_size=128):\n",
    "    transform_test = torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize(\n",
    "                (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root=\"data/cifar10\", train=False, download=True, transform=transform_test\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        testset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return test_loader\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        return 100.0 * correct / total\n",
    "    \n",
    "\n",
    "class Sampler:\n",
    "    \"\"\"Example of a sampler used for calibration.\"\"\"\n",
    "\n",
    "    def __init__(self, loader, device):\n",
    "        self.device = device\n",
    "        self.loader = iter(loader)\n",
    "        self.idx = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        x, _ = next(self.loader)\n",
    "        self.idx += 1\n",
    "        if self.idx > 100:\n",
    "            raise StopIteration\n",
    "\n",
    "        return ([x.to(self.device)], {})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, analogously to the 'hw_aware_training.ipynb' tutorial, we will set the RPU configuration. To do so, we define a new class whose features are mainly taken form [StandardHWATrainingPreset](../src/aihwkit/simulator/presets/inference.py)\n",
    "1. We set the ***mapping*** parameters, responsible for the mapping of the weights to the crossbar.\n",
    "2. We set the ***forward*** parameters, responsible for the non-idealities at tile level.\n",
    "3. We set the ***remap*** and ***clip*** parameters, responsible for the remapping ad clipping of the weights to the crossbar at each weight update during training, to ensure the weights are correctly mapped to appropriate conductances.\n",
    "4. We set the ***drift*** parameters, responsible for the drift of the weights.\n",
    "5. We set the ***pre-post*** parameters, responsible for the input range learning.\n",
    "6. We set the ***noise_model*** parameters, to characterize the read noise for the weights in the device.\n",
    "7. We set the ***modifier*** parameters, responsible for noise injection and as well as for the discretization of the weights at training time. We set the ***modifier*** to ***DISCRETIZE*** and enable it during the test phase. Also set the ***res*** parameter to 0.5 which correspondes to the resolution of the weights:\n",
    "    - rpu_config.modifier.type = DISCRETIZE\n",
    "    - enable_during_test = True \n",
    "### NOTE: \n",
    "the weights are discretized starting from the digital fp weights each minibatch: this means that, in case the value of a weight shifts form one quantization range to another, for example due to drift phenomena, the weight 'actual' quantized value will be updated only in the next minibatch (maybe, given the nature of drift effect, diluted over several minibatchs, this would not consistute such a big issue? Things may change at highter temperatures?) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of RPUConfig parameters\n",
    "\n",
    "@dataclass\n",
    "class CustomDefinedPreset(InferenceRPUConfig):\n",
    "\n",
    "    mapping: MappingParameter = field(\n",
    "        default_factory=lambda: MappingParameter(\n",
    "            weight_scaling_omega=1.0,\n",
    "            weight_scaling_columnwise=True,\n",
    "            max_input_size=512,\n",
    "            max_output_size=0,\n",
    "            digital_bias=True,\n",
    "            learn_out_scaling=True,\n",
    "            out_scaling_columnwise=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    forward: IOParameters = field(\n",
    "        default_factory=lambda: PresetIOParameters(\n",
    "            inp_res=254.0,\n",
    "            out_res=254.0,\n",
    "            bound_management=BoundManagementType.NONE,\n",
    "            noise_management=NoiseManagementType.CONSTANT,\n",
    "            nm_thres=1.0,\n",
    "            w_noise=0.0175,\n",
    "            w_noise_type=WeightNoiseType.PCM_READ,\n",
    "            ir_drop=1.0,\n",
    "            out_noise=0.04,\n",
    "            out_bound=10.0,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    remap: WeightRemapParameter = field(\n",
    "        default_factory=lambda: WeightRemapParameter(\n",
    "            remapped_wmax=1.0, type=WeightRemapType.CHANNELWISE_SYMMETRIC\n",
    "        )\n",
    "    )\n",
    "\n",
    "    noise_model: BaseNoiseModel = field(default_factory=PCMLikeNoiseModel)\n",
    "\n",
    "    drift_compensation: Optional[BaseDriftCompensation] = field(\n",
    "        default_factory=GlobalDriftCompensation\n",
    "    )\n",
    "\n",
    "    pre_post: PrePostProcessingParameter = field(\n",
    "        default_factory=lambda: PrePostProcessingParameter(\n",
    "            input_range=InputRangeParameter(\n",
    "                enable=True,\n",
    "                init_value=3.0,\n",
    "                init_from_data=100,\n",
    "                init_std_alpha=3.0,\n",
    "                decay=0.001,\n",
    "                input_min_percentage=0.95,\n",
    "                output_min_percentage=0.95,\n",
    "                manage_output_clipping=False,\n",
    "                gradient_scale=1.0,\n",
    "                gradient_relative=True,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    clip: WeightClipParameter = field(\n",
    "        default_factory=lambda: WeightClipParameter(\n",
    "            type=WeightClipType.FIXED_VALUE, fixed_value=1.0\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "    ''' --------------- MODIFY THIS PART ---------------'''\n",
    "\n",
    "    modifier: WeightModifierParameter = field(\n",
    "        default_factory=lambda: WeightModifierParameter(\n",
    "            std_dev=0.06,\n",
    "            type=WeightModifierType.DISCRETIZE,\n",
    "            res=0.5,\n",
    "            enable_during_test=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    ''' ------------------------------------------------'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /Users/edoardocabiati/Desktop/Cose brutte (PoliMI)/_tesi/aihwkit/sandbox/resnet9s.th\n",
      "Files already downloaded and verified\n",
      " Sample dimensions:  torch.Size([32, 32, 3])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "========================================================================================================================================================================================================\n",
       "Model Name: AnalogWrapperResnet9\n",
       "========================================================================================================================================================================================================\n",
       "Per-layer Information\n",
       "========================================================================================================================================================================================================\n",
       "Layer Information                                                     | Tile Information              \n",
       "========================================================================================================================================================================================================\n",
       "Layer Name          Is Analog           In Shape            Out Shape           Kernel Shape        # of Tiles          Reuse Factor        Log. tile shape     Phys. tile shape    utilization (%)     \n",
       "AnalogConv2d        analog              [1, 3, 32, 32]      [1, 28, 32, 32]     (3, 3)              1                   1024                -                   -                   -                   \n",
       "                                                                                                                                            (28, 27)            N/A                 100.00              \n",
       "BatchNorm2d         digital             [1, 28, 32, 32]     [1, 28, 32, 32]     -                   0                   0                   -                   -                   -                   \n",
       "BatchNorm2d         digital             [1, 28, 32, 32]     [1, 28, 32, 32]     -                   0                   0                   -                   -                   -                   \n",
       "ReLU                digital             [1, 28, 32, 32]     [1, 28, 32, 32]     -                   0                   0                   -                   -                   -                   \n",
       "AnalogConv2d        analog              [1, 28, 32, 32]     [1, 28, 32, 32]     (3, 3)              1                   1024                -                   -                   -                   \n",
       "                                                                                                                                            (28, 252)           N/A                 100.00              \n",
       "BatchNorm2d         digital             [1, 28, 32, 32]     [1, 28, 32, 32]     -                   0                   0                   -                   -                   -                   \n",
       "BatchNorm2d         digital             [1, 28, 32, 32]     [1, 28, 32, 32]     -                   0                   0                   -                   -                   -                   \n",
       "ReLU                digital             [1, 28, 32, 32]     [1, 28, 32, 32]     -                   0                   0                   -                   -                   -                   \n",
       "MaxPool2d           digital             [1, 28, 32, 32]     [1, 28, 16, 16]     2                   0                   0                   -                   -                   -                   \n",
       "AnalogConv2d        analog              [1, 28, 16, 16]     [1, 28, 16, 16]     (3, 3)              1                   256                 -                   -                   -                   \n",
       "                                                                                                                                            (28, 252)           N/A                 100.00              \n",
       "BatchNorm2d         digital             [1, 28, 16, 16]     [1, 28, 16, 16]     -                   0                   0                   -                   -                   -                   \n",
       "AnalogConv2d        analog              [1, 28, 16, 16]     [1, 28, 16, 16]     (3, 3)              1                   256                 -                   -                   -                   \n",
       "                                                                                                                                            (28, 252)           N/A                 100.00              \n",
       "BatchNorm2d         digital             [1, 28, 16, 16]     [1, 28, 16, 16]     -                   0                   0                   -                   -                   -                   \n",
       "AnalogSequential    analog              [1, 28, 16, 16]     [1, 28, 16, 16]     -                   0                   0                   -                   -                   -                   \n",
       "AnalogConv2d        analog              [1, 28, 16, 16]     [1, 28, 16, 16]     (3, 3)              1                   256                 -                   -                   -                   \n",
       "                                                                                                                                            (28, 252)           N/A                 100.00              \n",
       "BatchNorm2d         digital             [1, 28, 16, 16]     [1, 28, 16, 16]     -                   0                   0                   -                   -                   -                   \n",
       "BatchNorm2d         digital             [1, 28, 16, 16]     [1, 28, 16, 16]     -                   0                   0                   -                   -                   -                   \n",
       "ReLU                digital             [1, 28, 16, 16]     [1, 28, 16, 16]     -                   0                   0                   -                   -                   -                   \n",
       "MaxPool2d           digital             [1, 28, 16, 16]     [1, 28, 8, 8]       2                   0                   0                   -                   -                   -                   \n",
       "AnalogConv2d        analog              [1, 28, 8, 8]       [1, 56, 8, 8]       (3, 3)              1                   64                  -                   -                   -                   \n",
       "                                                                                                                                            (56, 252)           N/A                 100.00              \n",
       "BatchNorm2d         digital             [1, 56, 8, 8]       [1, 56, 8, 8]       -                   0                   0                   -                   -                   -                   \n",
       "BatchNorm2d         digital             [1, 56, 8, 8]       [1, 56, 8, 8]       -                   0                   0                   -                   -                   -                   \n",
       "ReLU                digital             [1, 56, 8, 8]       [1, 56, 8, 8]       -                   0                   0                   -                   -                   -                   \n",
       "MaxPool2d           digital             [1, 56, 8, 8]       [1, 56, 4, 4]       2                   0                   0                   -                   -                   -                   \n",
       "AnalogConv2d        analog              [1, 56, 4, 4]       [1, 56, 4, 4]       (3, 3)              1                   16                  -                   -                   -                   \n",
       "                                                                                                                                            (56, 504)           N/A                 100.00              \n",
       "BatchNorm2d         digital             [1, 56, 4, 4]       [1, 56, 4, 4]       -                   0                   0                   -                   -                   -                   \n",
       "AnalogConv2d        analog              [1, 56, 4, 4]       [1, 56, 4, 4]       (3, 3)              1                   16                  -                   -                   -                   \n",
       "                                                                                                                                            (56, 504)           N/A                 100.00              \n",
       "BatchNorm2d         digital             [1, 56, 4, 4]       [1, 56, 4, 4]       -                   0                   0                   -                   -                   -                   \n",
       "AnalogSequential    analog              [1, 56, 4, 4]       [1, 56, 4, 4]       -                   0                   0                   -                   -                   -                   \n",
       "MaxPool2d           digital             [1, 56, 4, 4]       [1, 56, 1, 1]       4                   0                   0                   -                   -                   -                   \n",
       "AnalogLinear        analog              [1, 56]             [1, 10]             -                   1                   1                   -                   -                   -                   \n",
       "                                                                                                                                            (10, 56)            N/A                 100.00              \n",
       "========================================================================================================================================================================================================\n",
       "General Information\n",
       "========================================================================================================================================================================================================\n",
       "Total number of tiles: 9\n",
       "Total number of analog layers: 11"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def setup():\n",
    "    # Create the model and load the weights\n",
    "    model = resnet9s().to(device)\n",
    "    download_url(\n",
    "        \"https://aihwkit-tutorial.s3.us-east.cloud-object-storage.appdomain.cloud/resnet9s.th\",\n",
    "        os.getcwd(),\n",
    "    )\n",
    "    state_dict = torch.load(\"resnet9s.th\", device)\n",
    "    # The state dict of the model with hardware-aware trained weights is stored in the\n",
    "    # model_state_dict key of the external checkpoint.\n",
    "    model.load_state_dict(state_dict[\"model_state_dict\"], strict=True)\n",
    "    model = convert_to_analog(model, CustomDefinedPreset())\n",
    "    model.eval()\n",
    "\n",
    "    # Load the test set\n",
    "    test_loader = get_test_loader()\n",
    "\n",
    "    # Plot specific dimension of a single instance in the test set and plot\n",
    "    # the sample\n",
    "    sample, _ = next(iter(test_loader))\n",
    "    sample = sample[0]\n",
    "    sample = sample.permute(1, 2, 0)\n",
    "    print(\" Sample dimensions: \", sample.shape)\n",
    "    # Standardize the sample values in the range [0, 1]\n",
    "    sample = (sample - sample.min()) / (sample.max() - sample.min())\n",
    "    plt.imshow(sample)\n",
    "    plt.show()\n",
    "\n",
    "    # Get a summary of the analog model\n",
    "    analog_summary(model, (1, sample.shape[2], sample.shape[0], sample.shape[1]), rpu_config=CustomDefinedPreset())\n",
    "\n",
    "    return model, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy Example\n",
    "Since the model obtained from the analog conversion method is a non subscriptable object (AnalogWrapper), we will define a toy example, made up of a single layer, to demonstrate the quantization of the weights using the rpu_config parameters set above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of the UNquantized model:\n",
      "Predicted:  tensor([[ 8.7029, 35.3072],\n",
      "        [ 8.3688, 35.5237]])\n",
      "Expected:  tensor([[10.4000, 30.5000],\n",
      "        [ 6.7000, 40.3000]])\n",
      "AnalogTile(RPUPulsed<float>[ConstantStep](2,4))\n",
      "Info about the tile (tensor([[-0.5021,  0.4799,  0.7674,  0.6651],\n",
      "        [ 0.5522,  0.3271, -0.8564,  0.4974]]), tensor([ 8.1570, 35.3260]))\n",
      "Evaluation of the quantized model:\n",
      "Predicted:  tensor([[ 8.7029, 35.2696],\n",
      "        [ 8.3970, 35.5237]])\n",
      "Expected:  tensor([[10.4000, 30.5000],\n",
      "        [ 6.7000, 40.3000]])\n",
      "Info about the tile (tensor([[-0.5021,  0.4799,  0.7674,  0.6651],\n",
      "        [ 0.5522,  0.3271, -0.8564,  0.4974]]), tensor([ 8.1570, 35.3260]))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "''' Taken form example '01_simple_layer.py' '''\n",
    "\n",
    "# Prepare the datasets (input and expected output).\n",
    "x = Tensor([[0.1, 0.2, 0.4, 0.3], [0.2, 0.1, 0.1, 0.3]])\n",
    "y = Tensor([[10.4, 30.5], [6.7, 40.3]])\n",
    "\n",
    "# Define a single-layer network, using a constant step device type.\n",
    "rpu_config = SingleRPUConfig(device=ConstantStepDevice())\n",
    "\n",
    "model = AnalogLinear(4, 2, bias=True, rpu_config=rpu_config)\n",
    "\n",
    "# Move the model and tensors to cuda if it is available.\n",
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    model = model.cuda()\n",
    "\n",
    "# Define an analog-aware optimizer, preparing it for using the layers.\n",
    "opt = AnalogSGD(model.parameters(), lr=0.1)\n",
    "opt.regroup_param_groups(model)\n",
    "\n",
    "# Train the model.\n",
    "for epoch in range(100):\n",
    "    # Delete old gradient\n",
    "    opt.zero_grad()\n",
    "    # Add the training Tensor to the model (input).\n",
    "    pred = model(x)\n",
    "    # Add the expected output Tensor.\n",
    "    loss = mse_loss(pred, y)\n",
    "    # Run training (backward propagation).\n",
    "    loss.backward()\n",
    "\n",
    "    opt.step()\n",
    "\n",
    "    #print(\"Loss error: {:.16f}\".format(loss))\n",
    "\n",
    "\n",
    "# Test the model\n",
    "print(\"Evaluation of the UNquantized model:\")\n",
    "model.eval()\n",
    "with no_grad():\n",
    "    pred = model(x)\n",
    "    print(\"Predicted: \", pred)\n",
    "    print(\"Expected: \", y)\n",
    "    analog_tile = next(model.analog_tiles())\n",
    "    print(analog_tile)\n",
    "    print(\"Info about the tile\", analog_tile.get_weights())\n",
    "\n",
    "\n",
    "# Generate a new model with weight discretization\n",
    "rpu_config.modifier = WeightModifierParameter(\n",
    "    std_dev=0.06,\n",
    "    type=WeightModifierType.DISCRETIZE,\n",
    "    res=0.5,\n",
    "    enable_during_test=True,\n",
    ")\n",
    "\n",
    "rpu_config.quantizer = WeightQuantizerParameter(\n",
    "    quantize = 0.5,\n",
    "    bound = 1.0,\n",
    ")\n",
    "\n",
    "model_new = convert_to_analog(model, rpu_config)\n",
    "# Import the model to the device\n",
    "model_new.load_state_dict(model_new.state_dict(), load_rpu_config=False)\n",
    "\n",
    "# Test the new model\n",
    "print(\"Evaluation of the quantized model:\")\n",
    "model_new.eval()\n",
    "with no_grad():\n",
    "    pred = model_new(x)\n",
    "    print(\"Predicted: \", pred)\n",
    "    print(\"Expected: \", y)\n",
    "    analog_tile = next(model.analog_tiles())\n",
    "    print(\"Info about the tile\", analog_tile.get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward:\n",
      "\t mv_type:\t\tOnePass\n",
      "\t inp/out_bound:\t\t1 / 12\n",
      "\t DAC/ADC:\t\t126 / 510\n",
      "\t out_noise:\t\t0.06\n",
      "\t noise_management:\t1\n",
      "\t bound_management [+/-]:Iterative\n",
      "Backward:\n",
      "\t mv_type:\t\tOnePass\n",
      "\t inp/out_bound:\t\t1 / 12\n",
      "\t DAC/ADC:\t\t126 / 510\n",
      "\t out_noise:\t\t0.06\n",
      "\t noise_management:\t1\n",
      "\t bound_management [+/-]:Iterative\n",
      "Update:\n",
      "\t desired_BL:\t\t31\n",
      "\t fixed_BL:\t\ttrue\n",
      "\t update_management:\ttrue\n",
      "\t update_management:\ttrue\n",
      "\t update_bl_management:\ttrue\n",
      "\t up_DAC_stoc_round:\tfalse\n",
      "\t up_DAC:\t\tinf\n",
      "\t pulse_type:\t\t1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(rpu_config.as_bindings())\n",
    "\n",
    "\n",
    "# t_inferences = [0.0, 3600.0, 86400.0]  # Times to perform infernece.\n",
    "# n_reps = 5  # Number of inference repetitions.\n",
    "# # Calibrate input ranges\n",
    "# print(\"Performing input range calibration\")\n",
    "# calibrate_input_ranges(\n",
    "#     model=model,\n",
    "#     calibration_type=InputRangeCalibrationType.CACHE_QUANTILE,\n",
    "#     dataloader=Sampler(test_loader, device),\n",
    "# )\n",
    "# # Determine the inference accuracy with the specified rpu configuration.\n",
    "# print(\"Evaluating imported model.\")\n",
    "# inference_accuracy_values = torch.zeros((len(t_inferences), n_reps))\n",
    "# for t_id, t in enumerate(t_inferences):\n",
    "#     for i in range(n_reps):\n",
    "#         model.drift_analog_weights(t)\n",
    "#         inference_accuracy_values[t_id, i] = evaluate_model(\n",
    "#             model, test_loader, device\n",
    "#         )\n",
    "\n",
    "#     print(\n",
    "#         f\"Test set accuracy (%) at t={t}s: mean: {inference_accuracy_values[t_id].mean()}, \\\n",
    "#             std: {inference_accuracy_values[t_id].std()}\"\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aihwkit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
